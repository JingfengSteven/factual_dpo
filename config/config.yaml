fact_tune_dir: /home/luokaiwei/workplace/ASC/factual_dpo

# random seed for batch sampling
seed: 0

# name for this experiment in the local run directory and on wandb
exp_name: test-1

# the batch size for training; for FSDP, the batch size per GPU is batch_size / (grad_accumulation_steps * num_gpus)
batch_size: 32

# the batch size during evaluation and sampling, if enabled
eval_batch_size: 32

# debug mode (disables wandb, model checkpointing, etc.)
debug: false

# the port to use for FSDP
fsdp_port: null

# which dataset(s) to train on; can pass a list like datasets=[hh,shp]
datasets:
- custom_dataset

data_prep:
  reward_mode: "factscore_avg"
  num_sft_targets: 10
  reward_diff_thresh: 0.0
  reward_version: null
  model_name_or_path: null 
  DEFAULT_SYSTEM_PROMPT: null
  DEFAULT_SYSTEM_PROMPT_LOC: ${fact_tune_dir}/system_prompt_concise.txt
  reward_file_train: ${fact_tune_dir}/data/dataset_bio/rewards/rewards_train_288.json
  reward_file_test: ${fact_tune_dir}/data/dataset_bio/rewards/rewards_dpoval_10.json"

summary_dir: ${fact_tune_dir}/eval/run_summaries

# evaluate and save model every eval_every steps
eval_every: 20000
#11250
#11205
#10361
#3834
#7574


# wandb configuration
wandb:
  enabled: true
  entity: null
  project: "factual_dpo"

# to create the local run directory and cache models/datasets,
#   we will try each of these directories in order; if none exist,
#   we will create the last one and use it
local_dirs:
  - /home/luokaiwei/model_zoo  # First priority: /dev/shm (plenty of space)
  #- .cache  # Fallback: .cache directory if /dev/shm is unavailable


# whether or not to generate samples during evaluation; disable for FSDP/TensorParallel
#   is recommended, because they are slow
sample_during_eval: true

# how many model samples to generate during evaluation
n_eval_model_samples: 8

# whether to eval at the very beginning of training
do_first_eval: true

# an OmegaConf resolver that returns the local run directory, calling a function in utils.py
local_run_dir: ${get_local_run_dir:${exp_name},${local_dirs}}

# the learning rate
lr: 1e-6

# number of steps to accumulate over for each batch
#   (e.g. if batch_size=4 and gradient_accumulation_steps=2, then we will
#   accumulate gradients over 2 microbatches of size 2)
gradient_accumulation_steps: 1

# the maximum gradient norm to clip to
max_grad_norm: 10.0


norm_type: 2

# the maximum allowed length for an input (prompt + response)
max_length: 1024

# the maximum allowed length for a prompt
max_prompt_length: 512

# the number of epochs to train for; if null, must specify n_examples
n_epochs: 1

# the number of examples to train for; if null, must specify n_epochs
n_examples: null

# the number of examples to evaluate on (and sample from, if sample_during_eval is true)
n_eval_examples: 256

# the trainer class to use (e.g. BasicTrainer, FSDPTrainer, TensorParallelTrainer)
trainer: BasicTrainer

# The optimizer to use; we use RMSprop because it works about as well as Adam and is more memory-efficient
optimizer: RMSprop 

# number of linear warmup steps for the learning rate
warmup_steps: 150

# whether or not to use activation/gradient checkpointing
activation_checkpointing: false

dataset_repeat: 25 # 1*1 per question, 1; 1*3 perquestion, 3 if 5*5 perquestion itis25
# prevent wandb from logging more than once per minimum_log_interval_secs
minimum_log_interval_secs: 1.0

defaults:
- _self_
- model: blank_model_fp32 # basic model configuration
- loss: dpo # which loss function, either sft or dpo (specify loss.beta if using dpo)
